---
# Ergatis Component Documentation
# Extracted from source by extract_component_docs.pl
# Source: TIGR/JCVI/IGS Ergatis bioinformatics workflow system
#
# This file preserves ALL documentation, comments, parameter descriptions,
# and structural details from the original Ergatis source files.

component: delete_by_bcp
extracted_at: "2026-02-19T15:34:47"
source_files:
  - delete_by_bcp.config
  - delete_by_bcp.xml

# Configuration file - all sections, parameters, defaults, and inline comments
config:
  interface:
    classification: database / manipulation
  parameters:
    DATABASE: $;PROJECT$;
    # The database type should describe the relational database management system e.g. sybase or postgresql
    DATABASE_TYPE: sybase
    # Server is the name of the server on which the database resides.
    SERVER: ""
    # For the time being the user does not have the ability to specify whether the delete SQL files will be
    # compressed.
    # -no_modify
    GZIP_SQL: 0
    # For the time being the user does not have the ability to specify whether the input BCP files are compressed.
    # The software expects the input BCP files to NOT be in a compressed state.
    # -no_modify
    GZIP_BCP: 0
  input:
    INPUT_FILE_LIST: ""
    INPUT_FILE: ""
    INPUT_DIRECTORY: ""
    # the following is only used when iterating over an INPUT_DIRECTORY
    INPUT_EXTENSION: bcp.out
  output delete_by_bcp:
    OUTPUT_TOKEN: default
    OUTPUT_DIRECTORY: $;REPOSITORY_ROOT$;/output_repository/$;COMPONENT_NAME$;/$;PIPELINEID$;_$;OUTPUT_TOKEN$;
    BCP_OUTPUT_LIST: $;OUTPUT_DIRECTORY$;/$;COMPONENT_NAME$;.bcp.list
  component:
    COMPONENT_NAME: delete_by_bcp
    DESCRIPTION: none
    WORKFLOW_REPOSITORY: $;REPOSITORY_ROOT$;/workflow/runtime/$;COMPONENT_NAME$;/$;PIPELINEID$;_$;OUTPUT_TOKEN$;
    PIPELINE_TOKEN: unnamed
    # The version,revision,tag here is set by an interpolated CVS tag
    VERSION: 2.0
    RELEASE_TAG: $Name$
    REVISION: "$Revision: 4680 $"
    TEMPLATE_XML: $;DOCS_DIR$;/$;COMPONENT_NAME$;.xml
    ITERATOR1: parse
    ITERATOR1_XML: $;DOCS_DIR$;/$;COMPONENT_NAME$;.$;ITERATOR1$;.xml
    ITERATOR2: dropindices
    ITERATOR2_XML: $;DOCS_DIR$;/$;COMPONENT_NAME$;.sqlforce.xml
    ITERATOR3: deleterecords
    ITERATOR3_XML: $;DOCS_DIR$;/$;COMPONENT_NAME$;.$;ITERATOR3$;.xml
    ITERATOR4: restoreindices
    ITERATOR4_XML: $;DOCS_DIR$;/$;COMPONENT_NAME$;.sqlforce.xml
    # Distributed options
    GROUP_COUNT: 150
    NODISTRIB: 0
    # the following keys are replaced at runtime by the invocation script
    COMPONENT_CONFIG: ""
    COMPONENT_XML: ""
    PIPELINE_XML: ""
    PIPELINEID: ""
  include:
    PROJECT_CONFIG: ""

# Main workflow XML template - step definitions
workflow_xml:
  xml_comments:
    - Preprocessing
    - Processing
    - "This iterator will read in the input file, list file or directory and create a list of BCP files and then also create the delete_records .sql files."
    - "This iterator will read drop_indexes.list and execute sql2DB.pl to delete all indices and constraints, but will preserve the primary keys."
    - The following step will create the delete_records.sql.list file for ITERATOR3 to process.
    - This iterator will read $;TMP_DIR$;/delete_records.sql.list and will execute sql2DB.pl to drop the records from chado tables.
    - "This iterator will read create_indexes.list and execute sql2DB.pl to restore all indices and constraints, excluding the primary keys (which weren't dropped in the first place)."
  command_sets:
    - delete_by_bcp workflow
  includes:
    - "file=\"$;DOCS_DIR$;/file_iterator_template.xml\" keys=\"$;ITERATOR_NAME$;=ITERATOR1,$;ITERATOR_XML$;=ITERATOR1_XML,$;ITERATOR_TIMESTAMP$;=1\"/"
    - "file=\"$;DOCS_DIR$;/iterator_template.xml\" keys=\"$;ITERATOR_NAME$;=ITERATOR2,$;ITERATOR_XML$;=ITERATOR2_XML,$;ITERATOR_LIST$;=$;TMP_DIR$;/drop_indexes.list,$;NODISTRIB$;=1,$;GROUP_NUMBER$;=1\"/"
    - "file=\"$;DOCS_DIR$;/iterator_template.xml\" keys=\"$;ITERATOR_NAME$;=ITERATOR3,$;ITERATOR_XML$;=ITERATOR3_XML,$;ITERATOR_LIST$;=$;TMP_DIR$;/delete_records.sql.list,$;NODISTRIB$;=1,$;GROUP_NUMBER$;=1\"/"
    - "file=\"$;DOCS_DIR$;/iterator_template.xml\" keys=\"$;ITERATOR_NAME$;=ITERATOR4,$;ITERATOR_XML$;=ITERATOR4_XML,$;ITERATOR_LIST$;=$;TMP_DIR$;/create_indexes.list,$;NODISTRIB$;=1,$;GROUP_NUMBER$;=1\"/"
  steps:
    - name: create output directory
      type: RunUnixCommand
      executable: mkdir
      arg: -p -m 777 $;OUTPUT_DIRECTORY$;
    - name: create temp directory
      type: RunUnixCommand
      executable: mkdir
      arg: -p -m 777 $;TMP_DIR$;
    - name: Drop indexes
      type: RunUnixCommand
      executable: $;BIN_DIR$;/ddl2iterator
      params:
        - key: --exclude_primary_keys
          value: 1
        - key: stdin
          value: $;DOCS_DIR$;/ddls/$;DATABASE_TYPE$;/drop_indexes.$;DATABASE_TYPE$;.ddl
        - key: stdout
          value: $;TMP_DIR$;/drop_indexes.list
    - name: create delete records file list
      type: RunUnixCommand
      executable: $;BIN_DIR$;/create_file_iterator_list
      params:
        - key: --input_directory
          value: $;TMP_DIR$;
        - key: --input_directory_extension
          value: sql
        - key: --output_iter_list
          value: $;TMP_DIR$;/delete_records.sql.list
        - key: stdout
          value: $;TMP_DIR$;/create_file_iterator_list.pl.stdout
        - key: stderr
          value: $;TMP_DIR$;/create_file_iterator_list.pl.stderr
    - name: Rebuild indexes
      type: RunUnixCommand
      executable: $;BIN_DIR$;/ddl2iterator
      params:
        - key: --exclude_primary_keys
          value: 1
        - key: stdin
          value: $;DOCS_DIR$;/ddls/$;DATABASE_TYPE$;/create_indexes.$;DATABASE_TYPE$;.ddl
        - key: stdout
          value: $;TMP_DIR$;/create_indexes.list

# Perl script documentation (POD)
perl_scripts:
  - script: create_file_iterator_list.pl
    pod: |
  =head1  NAME 
  
  create_file_iterator_list.pl - generates list file from various input sources of filenames.
  
  =head1 SYNOPSIS
  
  USAGE: create_file_iterator_list.pl 
          --input_file_list=/path/to/some.list
          --input_file=/path/to/somefile.fsa
          --input_directory=/path/to/some/dir
          --input_directory_extension=fsa
          [--checksum_filenames=1
          --log=/path/to/some.log
          --debug=4 ]
  
  =head1 OPTIONS
  
  B<--input_file_list,-l> 
      a plain text file containing the full paths to any number of files, one per line.  
      this can also be a comma-separated list of input file lists.
  
  B<--input_file,-f> 
      the full path to an input file. this can also be a comma-separated list of 
      input files.
  
  B<--input_directory,-d> 
      the full path to an input directory. this can also be a comma-separated list of 
      input directories.
  
  B<--input_directory_extension,-x> 
      to be used in conjuction with the input_directory option, this can be used to
      filter files by extension within any passed input directories.
  
  B<--output_iterator_list,-o>
      output file. Up to 7 lines with comma separated lists of limits
  
  B<--output_iterator_list,-o>
      comma separated list of iterator keys used in the output file.  Each key will be a separate line in the output file.  There are 7 lines in the output file listing FILE, FILE_NAME, FILE_BASE, FILE_EXT, DIRECTORY, XML file
  
  B<--checksum_filenames> 
      use checksums instead of the basename as the iterator name. The checksum will be based on the full path to the file.
  
  B<--log> 
      optional.  path to a log file the script should create.  will be overwritten if
      already exists.
      
  B<--debug> 
      optional.  the debug level for the logger (an integer)
  
  B<--help,-h> 
      This help message/documentation.
  
  =head1   DESCRIPTION
  
      This script is used to accept a selection of inputs from either an input list, 
      directory, file, or any combination thereof.  Each of these options can also
      be specified using comma-separated lists.  These inputs will then be distributed
      randomly across a certain number of groups, specified using the --group_count
      option (usually somewhere around 150).
      
      To prevent too many files from being directly in one group, additional groups are 
      automatically created for large input sets.  You can control how many inputs are 
      in a group before another is created using the --group_size_limit option (default 
      1000).  Once the number of inputs in any group reaches this number, another group 
      is created until it is full, and so on.
  
  =head1   OUTPUT
  
      The location of the output is specified using the --output_directory option.  One
      numbered directory will be created under this for each group needed.
      
      MORE OUTPUT DESCRIPTIONS NEEDED
      
  =head1 CONTACT
  
      Joshua Orvis
      jorvis@tigr.org
